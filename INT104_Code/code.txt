freq = 1;
decay = 5;
duration = 0.4;
retard = textIndex*thisComp.frameDuration*1;
t = time - (inPoint + retard);
startVal = [100,100,100];
endVal = [0,0,0];
if (t < duration){
linear(t,0,duration,startVal,endVal);
}else{
amp = (endVal - startVal)/duration;
w = freq*Math.PI*2;
endVal + amp*(Math.sin((t-duration)*w)/Math.exp(decay*(t-duration))/w);
}

// 这里
S_freq = 1;
S_decay = 5;
S_amp = 1;
S_duration = 0.25
S_startDelay = 0;
startVal = [0,0,0];
retard = (textTotal-textIndex+1)*thisComp.frameDuration*S_amp;
t = (outPoint-retard) - time - S_startDelay;
if(t < S_duration){
linear(t,0,S_duration,startVal,endVal);
}else{
amp = (endVal - startVal)/S_duration;
w = S_freq*Math.PI*2;
endVal + amp*(Math.sin((t-S_duration)*w)/Math.exp(S_decay*(t-S_duration))/w);
}




import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 1. 加载数据
data = pd.read_csv('your_data_file.csv') # 请替换为实际数据文件名

# 2. 数据预处理
# 删除异常值（标签为2的数据）
data_cleaned = data[data['label'] != 2]

# 分离特征和标签
X = data_cleaned.drop('label', axis=1)
y = data_cleaned['label']

# 特征缩放
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. 降维 - PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# 计算累计解释方差比
explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)

# 数据可视化（可选）
import matplotlib.pyplot as plt

plt.plot(range(1, len(explained_variance_ratio_cumsum) + 1), explained_variance_ratio_cumsum)
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

# 4. 选择合适数量的主成分
n_components = 0
for idx, cum_var in enumerate(explained_variance_ratio_cumsum):
    if cum_var >= 0.95:  # 保留95%的解释方差
        n_components = idx + 1
        break

# 5. 使用选择的主成分数量重新运行PCA
pca_final = PCA(n_components=n_components)
X_pca_final = pca_final.fit_transform(X_scaled)

# 至此，X_pca_final就是降维后的数据，可以用于接下来的分类任务。



# 没用的可视化

plt.figure(figsize=(10, 8))
plt.scatter(X_pca_final[:, 0], X_pca_final[:, 1], c=labels, cmap='viridis')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA Visualization of the Data')
plt.colorbar()
plt.show()

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_pca_final[:, 0], X_pca_final[:, 1], X_pca_final[:, 2], c=y)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
plt.show()




X_train, X_test, y_train, y_test = train_test_split(X_pca_final, y, test_size=0.2, random_state=42)

# SVM
svm = SVC(kernel='linear', C=1, random_state=42)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# DT
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("Classification Report:\n", classification_report(y_test, y_pred_dt))

# RF
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))


# 绘制热力图
def plot_confusion_matrix(y_true, y_pred, title):
    matrix = confusion_matrix(y_true, y_pred)
    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', square=True)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(title)


plt.figure(figsize=(15, 5))
plt.subplot(131)
plot_confusion_matrix(y_test, y_pred_svm, "SVM Confusion Matrix")
plt.subplot(132)
plot_confusion_matrix(y_test, y_pred_dt, "Decision Tree Confusion Matrix")
plt.subplot(133)
plot_confusion_matrix(y_test, y_pred_rf, "Random Forest Confusion Matrix")
plt.show()

#  绘制分类器的准确率比较柱状图
accuracies = [
    accuracy_score(y_test, y_pred_svm),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_rf),
]

classifiers = ["SVM", "Decision Tree", "Random Forest"]

plt.figure(figsize=(8, 6))
sns.barplot(x=classifiers, y=accuracies)
plt.xlabel("Classifier")
plt.ylabel("Accuracy")
plt.title("Classifier Accuracy Comparison")
plt.show()


# 绘制ROC曲线
def plot_roc_curve(y_true, y_score, label, color):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=color, lw=2, label=f"{label} (AUC = {roc_auc:.2f})")


svm_score = svm.decision_function(X_test)
dt_score = dt.predict_proba(X_test)[:, 1]
rf_score = rf.predict_proba(X_test)[:, 1]

plt.figure(figsize=(8, 6))
plot_roc_curve(y_test, svm_score, "SVM", "blue")
plot_roc_curve(y_test, dt_score, "Decision Tree", "green")
plot_roc_curve(y_test, rf_score, "Random Forest", "red")
plt.plot([0, 1], [0, 1], color="black", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(loc="lower right")
plt.show()



# dataValues = data.drop(['F2', 'F5', 'F6'], axis=1)
# dataValues = dataValues.mask(dataValues == 3, np.nan)
# dataValues = dataValues.mask(dataValues == 2, np.nan)
# dataValues = dataValues.fillna(dataValuesTem.mean())
# dataValues = dataValues.mask((dataValues == 2) | (dataValues == 3), dataValues.mean(),axis=0)
dimension = np.argmax(explained_variance_ratio_cumsum >= 0.95) + 1



#  星期一

# 创建并训练逻辑回归分类器
logistic_regression = LogisticRegression(random_state=42)
logistic_regression.fit(X_train, y_train)

# 对测试集进行预测
y_pred_lr = logistic_regression.predict(X_test)

# 评估逻辑回归分类器的性能
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("Classification Report:\n", classification_report(y_test, y_pred_lr))

# 添加逻辑回归分类器的准确率到之前的柱状图中
accuracies.append(accuracy_score(y_test, y_pred_lr))
classifiers.append("Logistic Regression")

# 更新柱状图
plt.figure(figsize=(10, 6))
sns.barplot(x=classifiers, y=accuracies)
plt.xlabel("Classifier")
plt.ylabel("Accuracy")
plt.title("Classifier Accuracy Comparison")
plt.show()

# 添加逻辑回归分类器的ROC曲线到之前的ROC曲线图中
lr_score = logistic_regression.predict_proba(X_test)[:, 1]
plot_roc_curve(y_test, lr_score, "Logistic Regression", "purple")
plt.legend(loc="lower right")
plt.show()


# MLP 多层感知器

# 创建并训练多层感知器（MLP）分类器
mlp = MLPClassifier(random_state=42, max_iter=1000, learning_rate_init=0.001)
mlp.fit(X_train, y_train)

# 对测试集进行预测
y_pred_mlp = mlp.predict(X_test)

# 评估多层感知器（MLP）分类器的性能
print("MLP Accuracy:", accuracy_score(y_test, y_pred_mlp))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_mlp))
print("Classification Report:\n", classification_report(y_test, y_pred_mlp))

# 创建并训练高斯朴素贝叶斯分类器
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# 对测试集进行预测
y_pred_gnb = gnb.predict(X_test)

# 评估高斯朴素贝叶斯分类器的性能
print("Gaussian Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_gnb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gnb))
print("Classification Report:\n", classification_report(y_test, y_pred_gnb))

accuracies.extend([
    accuracy_score(y_test, y_pred_mlp),
    accuracy_score(y_test, y_pred_gnb)
])
classifiers.extend([
    "MLP",
    "Gaussian Naive Bayes"
])

plt.figure(figsize=(12, 6))
sns.barplot(x=classifiers, y=accuracies)
plt.xlabel("Classifier")
plt.ylabel("Accuracy")
plt.title("Classifier Accuracy Comparison")
plt.show()

C=1
SVM Accuracy: 0.7120075046904315
Confusion Matrix:
 [[432 154]
 [153 327]]
Classification Report:
               precision    recall  f1-score   support
           0       0.74      0.74      0.74       586
           1       0.68      0.68      0.68       480
    accuracy                           0.71      1066
   macro avg       0.71      0.71      0.71      1066
weighted avg       0.71      0.71      0.71      1066

null
Decision Tree Accuracy: 0.649155722326454
Confusion Matrix:
 [[429 157]
 [217 263]]
Classification Report:
               precision    recall  f1-score   support
           0       0.66      0.73      0.70       586
           1       0.63      0.55      0.58       480
    accuracy                           0.65      1066
   macro avg       0.65      0.64      0.64      1066
weighted avg       0.65      0.65      0.65      1066

n_estimators=1000
Random Forest Accuracy: 0.7063789868667918
Confusion Matrix:
 [[437 149]
 [164 316]]
Classification Report:
               precision    recall  f1-score   support
           0       0.73      0.75      0.74       586
           1       0.68      0.66      0.67       480
    accuracy                           0.71      1066
   macro avg       0.70      0.70      0.70      1066
weighted avg       0.71      0.71      0.71      1066

C=0.1, solver='saga'
Logistic Regression Accuracy: 0.7317073170731707
Confusion Matrix:
 [[446 140]
 [146 334]]
Classification Report:
               precision    recall  f1-score   support
           0       0.75      0.76      0.76       586
           1       0.70      0.70      0.70       480
    accuracy                           0.73      1066
   macro avg       0.73      0.73      0.73      1066
weighted avg       0.73      0.73      0.73      1066



Logistic Regression Train Accuracy: 0.7202157598499062
SVM Cross Validation Scores: [0.69793621 0.68761726 0.7195122  0.72326454 0.63227017]
SVM Cross Validation Mean Accuracy: 0.6921200750469043
Decision Tree Cross Validation Scores: [0.63789869 0.62101313 0.67260788 0.63977486 0.59287054]
Decision Tree Cross Validation Mean Accuracy: 0.6328330206378986
Random Forest Cross Validation Scores: [0.66697936 0.66510319 0.70262664 0.68667917 0.59474672]
Random Forest Cross Validation Mean Accuracy: 0.6632270168855536
Logistic Regression Cross Validation Scores: [0.68292683 0.68855535 0.73452158 0.7195122  0.61726079]
Logistic Regression Cross Validation Mean Accuracy: 0.6885553470919324